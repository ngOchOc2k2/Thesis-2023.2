{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Param\n",
    "\n",
    "param = Param()\n",
    "args = param.args\n",
    "args\n",
    "\n",
    "args.task_name = args.dataname\n",
    "\n",
    "# rel_per_task\n",
    "args.rel_per_task = 8 if args.dataname == \"FewRel\" else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu=0, dataname='TACRED', task_name='TACRED', device='cuda', batch_size=64, num_tasks=10, rel_per_task=4, pattern='entity_marker', max_length=192, encoder_output_size=768, vocab_size=30522, marker_size=4, num_workers=0, save_checkpoint='./checkpoint/', classifier_epochs=10, seed=2021, max_grad_norm=10, data_path='/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/datasets/standard/', bert_path='bert-base-uncased', drop_p=0.1, gradient_accumulation_steps=4, total_round=6, drop_out=0.5, use_gpu=True, hidden_size=768, rank_lora=8, bge_model='/kaggle/working/model_bge/checkpoint-6000', description_path='/kaggle/input/train-bge/standard.json', type_similar='colbert', num_protos=10)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import json, os\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np  \n",
    "\n",
    "\n",
    "def get_tokenizer(args):\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_path, additional_special_tokens=[\"[E11]\", \"[E12]\", \"[E21]\", \"[E22]\"])\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "class data_sampler(object):\n",
    "    def __init__(self, args, seed=None):\n",
    "        self.set_path(args)\n",
    "        self.args = args\n",
    "\n",
    "        # data path\n",
    "        file_name = \"{}.pkl\".format(\"-\".join([str(x) for x in [args.dataname, args.seed]]))\n",
    "        mid_dir = \"\"\n",
    "        for temp_p in [\"datasets\", \"_process_path\"]:\n",
    "            mid_dir = os.path.join(mid_dir, temp_p)\n",
    "            if not os.path.exists(mid_dir):\n",
    "                os.mkdir(mid_dir)\n",
    "        self.save_data_path = os.path.join(mid_dir, file_name)\n",
    "\n",
    "        # import tokenizer\n",
    "        self.tokenizer = get_tokenizer(args)\n",
    "\n",
    "        # read relation data\n",
    "        self.id2rel, self.rel2id = self._read_relations(args.relation_file)\n",
    "\n",
    "        # random sampling\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            random.seed(self.seed)\n",
    "        self.shuffle_index = list(range(len(self.id2rel)))\n",
    "        random.shuffle(self.shuffle_index)\n",
    "        self.shuffle_index = np.argsort(self.shuffle_index)\n",
    "\n",
    "        # regenerate data\n",
    "        self.training_dataset, self.valid_dataset, self.test_dataset = self._read_data(self.args.data_file)\n",
    "\n",
    "        # generate the task number\n",
    "        self.batch = 0\n",
    "        self.task_length = len(self.id2rel) // self.args.rel_per_task\n",
    "\n",
    "        # record relations\n",
    "        self.seen_relations = []\n",
    "        self.history_test_data = {}\n",
    "        \n",
    "        if args.dataname in [\"FewRel\"]:\n",
    "            self.id2rel = json.load(open(os.path.join(args.data_path, \"id2rel.json\"), 'r'))\n",
    "        else:\n",
    "            self.id2rel = json.load(open(os.path.join(args.data_path, \"id2rel_tacred.json\"), 'r'))\n",
    "        \n",
    "        self.rel2id = {label: idx for idx, label in enumerate(self.id2rel)}\n",
    "        \n",
    "\n",
    "    def set_path(self, args):\n",
    "        use_marker = \"\"\n",
    "        if args.dataname in [\"FewRel\"]:\n",
    "            args.data_file = os.path.join(args.data_path, \"data_with{}_marker.json\".format(use_marker))\n",
    "            args.relation_file = os.path.join(args.data_path, \"id2rel.json\")\n",
    "            args.num_of_relation = 80\n",
    "            args.num_of_train = 420\n",
    "            args.num_of_val = 140\n",
    "            args.num_of_test = 140\n",
    "            \n",
    "        elif args.dataname in [\"TACRED\"]:\n",
    "            args.data_file = os.path.join(args.data_path, \"data_with{}_marker_tacred.json\".format(use_marker))\n",
    "            args.relation_file = os.path.join(args.data_path, \"id2rel_tacred.json\")\n",
    "            args.num_of_relation = 40\n",
    "            args.num_of_train = 420\n",
    "            args.num_of_val = 140\n",
    "            args.num_of_test = 140\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        self.seed = seed\n",
    "        if self.seed != None:\n",
    "            random.seed(self.seed)\n",
    "        self.shuffle_index = list(range(len(self.id2rel)))\n",
    "        random.shuffle(self.shuffle_index)\n",
    "        self.shuffle_index = np.argsort(self.shuffle_index)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.batch == self.task_length:\n",
    "            raise StopIteration()\n",
    "\n",
    "        indexs = self.shuffle_index[self.args.rel_per_task * self.batch : self.args.rel_per_task * (self.batch + 1)]\n",
    "        self.batch += 1\n",
    "\n",
    "        current_relations = []\n",
    "        cur_training_data = {}\n",
    "        cur_valid_data = {}\n",
    "        cur_test_data = {}\n",
    "\n",
    "        for index in indexs:\n",
    "            current_relations.append(self.id2rel[index])\n",
    "            self.seen_relations.append(self.id2rel[index])\n",
    "            cur_training_data[self.id2rel[index]] = self.training_dataset[index]\n",
    "            cur_valid_data[self.id2rel[index]] = self.valid_dataset[index]\n",
    "            cur_test_data[self.id2rel[index]] = self.test_dataset[index]\n",
    "            self.history_test_data[self.id2rel[index]] = self.test_dataset[index]\n",
    "\n",
    "        return cur_training_data, cur_valid_data, cur_test_data, current_relations, self.history_test_data, self.seen_relations\n",
    "\n",
    "    def _read_data(self, file):\n",
    "        if os.path.isfile(self.save_data_path):\n",
    "            with open(self.save_data_path, \"rb\") as f:\n",
    "                datas = pickle.load(f)\n",
    "            train_dataset, val_dataset, test_dataset = datas\n",
    "            return train_dataset, val_dataset, test_dataset\n",
    "        else:\n",
    "            data = json.load(open(file, \"r\", encoding=\"utf-8\"))\n",
    "            train_dataset = [[] for i in range(self.args.num_of_relation)]\n",
    "            val_dataset = [[] for i in range(self.args.num_of_relation)]\n",
    "            test_dataset = [[] for i in range(self.args.num_of_relation)]\n",
    "            for relation in data.keys():\n",
    "                rel_samples = data[relation]\n",
    "                if self.seed != None:\n",
    "                    random.seed(self.seed)\n",
    "                random.shuffle(rel_samples)\n",
    "                count = 0\n",
    "                count1 = 0\n",
    "                for i, sample in enumerate(rel_samples):\n",
    "                    tokenized_sample = {}\n",
    "                    tokenized_sample[\"relation\"] = self.rel2id[sample[\"relation\"]]\n",
    "                    tokenized_sample[\"text\"] = \" \".join(sample[\"tokens\"])\n",
    "                    tokenized_sample[\"tokens\"] = self.tokenizer.encode(\" \".join(sample[\"tokens\"]), padding=\"max_length\", truncation=True, max_length=self.args.max_length)\n",
    "\n",
    "\n",
    "                    if self.args.task_name == \"FewRel\":\n",
    "                        if i < self.args.num_of_train:\n",
    "                            train_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                        elif i < self.args.num_of_train + self.args.num_of_val:\n",
    "                            val_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                        else:\n",
    "                            test_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                    else:\n",
    "                        if i < len(rel_samples) // 5 and count <= 40:\n",
    "                            count += 1\n",
    "                            test_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                        else:\n",
    "                            count1 += 1\n",
    "                            train_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                            if count1 >= 320:\n",
    "                                break\n",
    "\n",
    "                    \n",
    "            with open(self.save_data_path, \"wb\") as f:\n",
    "                pickle.dump((train_dataset, val_dataset, test_dataset), f)\n",
    "            return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def _read_relations(self, file):\n",
    "        id2rel = json.load(open(file, \"r\", encoding=\"utf-8\"))\n",
    "        rel2id = {}\n",
    "        for i, x in enumerate(id2rel):\n",
    "            rel2id[x] = i\n",
    "        return id2rel, rel2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relation': 3, 'text': \"When [E11] he [E12] rode streetcars as a child in [E21] Shreveport [E22] , he often sat in the back but found that the conductors would exclude him by moving the `` blacks only '' sign behind him so he would be in the white section .\", 'tokens': [101, 2043, 30522, 2002, 30523, 8469, 21420, 2015, 2004, 1037, 2775, 1999, 30524, 23740, 30525, 1010, 2002, 2411, 2938, 1999, 1996, 2067, 2021, 2179, 2008, 1996, 23396, 2052, 23329, 2032, 2011, 3048, 1996, 1036, 1036, 10823, 2069, 1005, 1005, 3696, 2369, 2032, 2061, 2002, 2052, 2022, 1999, 1996, 2317, 2930, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 32, 'text': '[E11] Kurt Waldheim [E12] , the former UN secretary general and president of Austria whose reputation was tarnished by revelations of his Nazi past , died Thursday from a [E21] heart attack [E22] at the age of 88 , his family said .', 'tokens': [101, 30522, 9679, 24547, 16425, 12112, 30523, 1010, 1996, 2280, 4895, 3187, 2236, 1998, 2343, 1997, 5118, 3005, 5891, 2001, 16985, 28357, 2011, 22191, 1997, 2010, 6394, 2627, 1010, 2351, 9432, 2013, 1037, 30524, 2540, 2886, 30525, 2012, 1996, 2287, 1997, 6070, 1010, 2010, 2155, 2056, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 30, 'text': \"`` We all are hopeful and we trust these judges and the jury to know that they are going to not put two innocent kids in jail for a crime that they did n't commit , '' [E11] Knox [E12] 's mother , [E21] Edda Mellas [E22] , told reporters after the hearing .\", 'tokens': [101, 1036, 1036, 2057, 2035, 2024, 17772, 1998, 2057, 3404, 2122, 6794, 1998, 1996, 6467, 2000, 2113, 2008, 2027, 2024, 2183, 2000, 2025, 2404, 2048, 7036, 4268, 1999, 7173, 2005, 1037, 4126, 2008, 2027, 2106, 1050, 1005, 1056, 10797, 1010, 1005, 1005, 30522, 11994, 30523, 1005, 1055, 2388, 1010, 30524, 3968, 2850, 11463, 8523, 30525, 1010, 2409, 12060, 2044, 1996, 4994, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 10, 'text': '[E21] Virginia [E22] Governor Tim Kaine will schedule a special election to fill the remainder of [E11] her [E12] term .', 'tokens': [101, 30524, 3448, 30525, 3099, 5199, 11928, 2638, 2097, 6134, 1037, 2569, 2602, 2000, 6039, 1996, 6893, 1997, 30522, 2014, 30523, 2744, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 4, 'text': 'He is survived by his wife , Valerie , whom [E11] he [E12] married in 2008 ; a son , Jeremy ; and a daughter , [E21] Tiphaine [E22] , from a previous marriage .', 'tokens': [101, 2002, 2003, 5175, 2011, 2010, 2564, 1010, 14264, 1010, 3183, 30522, 2002, 30523, 2496, 1999, 2263, 1025, 1037, 2365, 1010, 7441, 1025, 1998, 1037, 2684, 1010, 30524, 5955, 10932, 2638, 30525, 1010, 2013, 1037, 3025, 3510, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 31, 'text': \"[E11] His [E12] bachelor 's degree in computer science and mathematics is from [E21] Sharif University [E22] in Iran .\", 'tokens': [101, 30522, 2010, 30523, 5065, 1005, 1055, 3014, 1999, 3274, 2671, 1998, 5597, 2003, 2013, 30524, 20351, 2118, 30525, 1999, 4238, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 18, 'text': 'Hi , [E11] Qatalys , Inc [E12] founded in 1995 and headquartered in Dallas , [E21] Texas [E22] with multiple offshore facilities in India .', 'tokens': [101, 7632, 1010, 30522, 1053, 27815, 7274, 1010, 4297, 30523, 2631, 1999, 2786, 1998, 9403, 1999, 5759, 1010, 30524, 3146, 30525, 2007, 3674, 12195, 4128, 1999, 2634, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 34, 'text': \"[E11] Maj Gen John P Henebry [E12] , who played a leading role in the Army Air Forces ' innovative low-level bombing of Japanese ships in the South Pacific during World War II , died Sept 30 in [E21] Evanston , Ill [E22] .\", 'tokens': [101, 30522, 16686, 8991, 2198, 1052, 21863, 15878, 2854, 30523, 1010, 2040, 2209, 1037, 2877, 2535, 1999, 1996, 2390, 2250, 2749, 1005, 9525, 2659, 1011, 2504, 8647, 1997, 2887, 3719, 1999, 1996, 2148, 3534, 2076, 2088, 2162, 2462, 1010, 2351, 17419, 2382, 1999, 30524, 6473, 2669, 1010, 5665, 30525, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 9, 'text': \"[E11] China Banking Regulatory Commission [E12] : [E21] http://wwwcbrcgovcn [E22] China 's banks avoided the mortgage-related turmoil that battered Western institutions but Chinese regulators are trying to improve risk management .\", 'tokens': [101, 30522, 2859, 8169, 10738, 3222, 30523, 1024, 30524, 8299, 1024, 1013, 1013, 7479, 27421, 11890, 3995, 25465, 2078, 30525, 2859, 1005, 1055, 5085, 9511, 1996, 14344, 1011, 3141, 17930, 2008, 17548, 2530, 4896, 2021, 2822, 25644, 2024, 2667, 2000, 5335, 3891, 2968, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 28, 'text': \"NEW YORK 2008-01-14 15:45:15 UTC In February 2000 several investors bought stakes of undisclosed size in [E11] Zagat [E12] , including [E21] General Atlantic LLC [E22] , a private equity firm ; the venture capital firm Kleiner Perkins Caufield & Byers ; investment bank Allen & Co as well as Nancy Peretsman , executive vice president at Allen ; former Microsoft Corp executive Nathan Myhrvold and Nicholas Negroponte , a computer scientist who founded the Massachusetts Institute of Technology 's Media Lab and the One Laptop Per Child project .\", 'tokens': [101, 2047, 2259, 2263, 1011, 5890, 1011, 2403, 2321, 1024, 3429, 1024, 2321, 11396, 1999, 2337, 2456, 2195, 9387, 4149, 7533, 1997, 18206, 2946, 1999, 30522, 23564, 20697, 30523, 1010, 2164, 30524, 2236, 4448, 11775, 30525, 1010, 1037, 2797, 10067, 3813, 1025, 1996, 6957, 3007, 3813, 12555, 2121, 13601, 6187, 16093, 12891, 1004, 9061, 2869, 1025, 5211, 2924, 5297, 1004, 2522, 2004, 2092, 2004, 7912, 23976, 3215, 2386, 1010, 3237, 3580, 2343, 2012, 5297, 1025, 2280, 7513, 13058, 3237, 7150, 2026, 8093, 6767, 6392, 1998, 6141, 12593, 26029, 2618, 1010, 1037, 3274, 7155, 2040, 2631, 1996, 4404, 2820, 1997, 2974, 1005, 1055, 2865, 6845, 1998, 1996, 2028, 12191, 2566, 2775, 2622, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "data = data_sampler(args, seed=2021)\n",
    "list_data = []\n",
    "\n",
    "for steps, (training_data, valid_data, test_data, current_relations, historic_test_data, seen_relations) in enumerate(data):\n",
    "    \n",
    "    # list_data.append(task_x)\n",
    "    \n",
    "        # for item in training_data[relation]:\n",
    "        #     if item['relation'] == 0:\n",
    "        #         print(item)\n",
    "    # if training_data[current_relations[0]][21]['relation'] == 0:\n",
    "    print(training_data[current_relations[0]][21])\n",
    "    # print(current_relations)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_data)\n",
    "\n",
    "json.dump(list_data, open('./prototype.json', 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "x = json.load(open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/hoang/train.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['per:cities_of_residence', 'per:other_family', 'org:founded', 'per:origin']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]['relations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The relation name \"org:stateorprovince_of_headquarters\" refers to the specific type of relationship between an organization (E11, E12) and the state or province where that organization's headquarters are located (E21, E22). This relationship is identified from textual data that explicitly or implicitly mentions the geographical location of the main office or central operating facility of an organization. The examples provided illustrate various instances where the headquarters of an organization are linked to a particular state or province, showcasing the diversity in organizational types and the geographical spread of their central operations.\n",
    "\n",
    "The \"org:stateorprovince_of_headquarters\" relation is pivotal for understanding the geographic distribution of organizational headquarters, which can offer insights into the regional economic impact, employment patterns, and the strategic decisions behind where companies choose to base their main operations. This relationship helps in mapping out the corporate landscape of different regions, indicating the concentration of industries, the preference of organizations for certain locales due to logistical, economic, or regulatory advantages, and the potential influence of these organizations on local economies and policies.\n",
    "\n",
    "Understanding this relation is crucial for economic analysis, regional planning, and the study of corporate strategies. It aids in identifying how organizations are distributed across states or provinces, which can be essential for supply chain logistics, marketing strategies, and understanding the regional dynamics of business operations. This relation also has implications for tax policies, infrastructure development, and workforce distribution, offering a lens through which to view the interactions between corporations and the geographical contexts of their headquarters.\n",
    "\"\"\"\n",
    "for item in x:\n",
    "    if item['relation'] == \"org:stateorprovince_of_headquarters\":\n",
    "        print(\"Yes\")\n",
    "        item['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = json.load(open('/home/luungoc/Thesis - 2023.2/tacred-relation/dataset/tacred/train.json', 'r'))\n",
    "\n",
    "obj_type = set()\n",
    "sub_type = set()\n",
    "\n",
    "for sample in data:\n",
    "    obj_type.add(sample['obj_type'])\n",
    "    sub_type.add(sample['subj_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_old = json.load(open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/datasets/standard/data_no_marker_tacred.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/luungoc/.cache/huggingface/datasets/xiaobendanyn___text/xiaobendanyn--tacred-6917f9984c48fec2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd72d98f2b054485a70253923caab44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_ori = load_dataset('xiaobendanyn/tacred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, valid_data, test_data = {}, {}, {}\n",
    "\n",
    "relations = []\n",
    "for item in data_ori['train']['text']:\n",
    "    relations.append(item)\n",
    "    \n",
    "    \n",
    "for relation in relations:\n",
    "    training_data[relation], test_data[relation], valid_data[relation] = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['train'][1]['text']\n",
    "\n",
    "\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def extract_entities(json_string):\n",
    "    # Chuyển chuỗi JSON thành dictionary\n",
    "    data = json.loads(json_string)\n",
    "    \n",
    "    # Lấy thông tin cần thiết từ dictionary\n",
    "    token = data.get('token', [])\n",
    "    relation = data.get('relation', None)\n",
    "    entity_1 = data.get('h', {}).get('name', None)\n",
    "    entity_2 = data.get('t', {}).get('name', None)\n",
    "    \n",
    "    # Tạo dictionary mới với thông tin lấy được\n",
    "    new_data = {\n",
    "        'token': token,\n",
    "        'relation': relation,\n",
    "        'entity_1': entity_1,\n",
    "        'entity_2': entity_2\n",
    "    }\n",
    "    \n",
    "    return insert_entity_tokens(new_data)\n",
    "\n",
    "\n",
    "\n",
    "def insert_entity_tokens(data):\n",
    "    new = deepcopy(data)\n",
    "    token = data.get('token', [])\n",
    "    entity_1 = data.get('entity_1', None)\n",
    "    entity_2 = data.get('entity_2', None)\n",
    "\n",
    "    if entity_1:\n",
    "        # Tìm vị trí của entity_1 trong danh sách token\n",
    "        index_entity_1 = find_entity_index(token, entity_1)\n",
    "        if index_entity_1 is not None:\n",
    "            # Chèn [E11] và [E12] vào vị trí tương ứng trong danh sách token\n",
    "            token.insert(index_entity_1, \"[E11]\")\n",
    "            token.insert(index_entity_1 + len(entity_1.split()) + 1, \"[E12]\")\n",
    "\n",
    "    if entity_2:\n",
    "        # Tìm vị trí của entity_2 trong danh sách token\n",
    "        index_entity_2 = find_entity_index(token, entity_2)\n",
    "        if index_entity_2 is not None:\n",
    "            # Chèn [E21] và [E22] vào vị trí tương ứng trong danh sách token\n",
    "            token.insert(index_entity_2, \"[E21]\")\n",
    "            token.insert(index_entity_2 + len(entity_2.split()) + 1, \"[E22]\")\n",
    "\n",
    "    return {\n",
    "        'relation': data['relation'],\n",
    "        'token_with_marker': token,\n",
    "        'token': new['token'] \n",
    "    }\n",
    "\n",
    "def find_entity_index(token, entity):\n",
    "    entity_words = entity.split()\n",
    "    for i in range(len(token) - len(entity_words) + 1):\n",
    "        if token[i:i+len(entity_words)] == entity_words:\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['org:stateorprovince_of_headquarters',\n",
       " 'per:stateorprovince_of_death',\n",
       " 'per:cities_of_residence',\n",
       " 'org:top_members/employees',\n",
       " 'org:parents',\n",
       " 'per:origin',\n",
       " 'per:parents',\n",
       " 'per:religion',\n",
       " 'per:date_of_birth',\n",
       " 'org:dissolved',\n",
       " 'org:country_of_headquarters',\n",
       " 'per:charges',\n",
       " 'NA',\n",
       " 'per:city_of_birth',\n",
       " 'per:title',\n",
       " 'org:number_of_employees/members',\n",
       " 'per:stateorprovince_of_birth',\n",
       " 'per:age',\n",
       " 'org:founded_by',\n",
       " 'per:alternate_names',\n",
       " 'org:political/religious_affiliation',\n",
       " 'org:shareholders',\n",
       " 'org:founded',\n",
       " 'per:country_of_death',\n",
       " 'org:member_of',\n",
       " 'per:siblings',\n",
       " 'org:members',\n",
       " 'org:alternate_names',\n",
       " 'per:country_of_birth',\n",
       " 'per:children',\n",
       " 'per:countries_of_residence',\n",
       " 'org:website',\n",
       " 'org:city_of_headquarters',\n",
       " 'per:city_of_death',\n",
       " 'per:other_family',\n",
       " 'org:subsidiaries',\n",
       " 'per:spouse',\n",
       " 'per:date_of_death',\n",
       " 'per:schools_attended',\n",
       " 'per:employee_of',\n",
       " 'per:cause_of_death',\n",
       " 'per:stateorprovinces_of_residence']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, valid_data, test_data = {}, {}, {}\n",
    "\n",
    "    \n",
    "relations = set()\n",
    "\n",
    "for sample in data_ori['train']:\n",
    "    fix = extract_entities(sample['text'])\n",
    "    \n",
    "    relations.add(fix['relation'])\n",
    "    # training_data[fix['relation']].append(fix)\n",
    "    \n",
    "    \n",
    "    \n",
    "for relation in list(relations):\n",
    "    training_data[relation], test_data[relation], valid_data[relation] = [], [], []\n",
    "    \n",
    "    \n",
    "list(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106203"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, valid_data, test_data = {}, {}, {}\n",
    "\n",
    "    \n",
    "relations = set()\n",
    "\n",
    "for sample in data_ori['train']:\n",
    "    fix = extract_entities(sample['text'])\n",
    "    \n",
    "    relations.add(fix['relation'])\n",
    "    # training_data[fix['relation']].append(fix)\n",
    "    \n",
    "    \n",
    "    \n",
    "for relation in list(relations):\n",
    "    training_data[relation], test_data[relation], valid_data[relation] = [], [], []\n",
    "    \n",
    "\n",
    "for sample in data_ori['train']:\n",
    "    fix = extract_entities(sample['text'])\n",
    "    \n",
    "    training_data[fix['relation']].append(fix)\n",
    "\n",
    "\n",
    "for sample in data_ori['test']:\n",
    "    fix = extract_entities(sample['text'])\n",
    "    \n",
    "    test_data[fix['relation']].append(fix)\n",
    "\n",
    "\n",
    "for sample in data_ori['validation']:\n",
    "    fix = extract_entities(sample['text'])\n",
    "    \n",
    "    valid_data[fix['relation']].append(fix)\n",
    "    \n",
    "    \n",
    "total = {}\n",
    "count = 0\n",
    "\n",
    "for keys, values in enumerate(training_data):\n",
    "    total[values] = []\n",
    "    \n",
    "    if values != 'per:country_of_death':\n",
    "        total[values] += (training_data[values] + test_data[values] + valid_data[values])\n",
    "        count += len(total[values])\n",
    "        \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_na = []\n",
    "\n",
    "for sample in total['NA']:\n",
    "    relation_na.append({\n",
    "        'relation': sample['relation'],\n",
    "        'tokens': sample['token_with_marker'],\n",
    "    })\n",
    "    \n",
    "    if len(relation_na) == 500:\n",
    "        break\n",
    "\n",
    "json.dump(relation_na, open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/datasets/standard/no_relation.json', 'w'), ensure_ascii=False)\n",
    "\n",
    "# def find_index(arr, value):\n",
    "#     for i in range(len(arr)):\n",
    "#         if arr[i] == value:\n",
    "#             return i\n",
    "#     return -1\n",
    "        \n",
    "# for keys, values in enumerate(total):\n",
    "#     if values == 'NA':\n",
    "#         for sample in total[values]:\n",
    "#             if find_index(sample['token_with_marker'], '[E11]') + 1 == find_index(sample['token_with_marker'], '[E12]') \\\n",
    "#             or find_index(sample['token_with_marker'], '[E21]') + 1 == find_index(sample['token_with_marker'], '[E22]'):\n",
    "#                 print(sample['token_with_marker'])\n",
    "#             # print(find_index(sample['token_with_marker'], value='[E11]'))\n",
    "#             # print(sample['token_with_marker'], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marker, train_no_marker = {}, {}\n",
    "\n",
    "for keys, values in enumerate(total):\n",
    "    train_marker[values], train_no_marker[values] = [], []\n",
    "    \n",
    "    for sample in total[values]:\n",
    "        if sample['relation'] not in ['per:country_of_death', 'NA']:\n",
    "            train_marker[values].append({\n",
    "                'relation': sample['relation'],\n",
    "                'tokens': sample['token_with_marker'],\n",
    "            })\n",
    "            \n",
    "            train_no_marker[values].append({\n",
    "                'relation': sample['relation'],\n",
    "                'tokens': sample['token'],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_old = json.load(open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/datasets/train_step_{steps}.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the extracted passage, tokens [E11], [E12], [E21], [E22] appear to mark the positions of entities. However, the words or phrases between them are not linked or refer to any relationship between these entities.\n",
      "Example: Survivors include his wife of 58 years , Jane Callaghan Gude of Washington ; five children , Sharon Gude of Rockville , Adrienne Lewis of Washington , Gilbert Gude Jr. of Bethesda and [E11] Gregory Gude [E12] and [E21] Daniel Gude [E22] , both of Cabin John , Md. ; and three grandchildren .\n"
     ]
    }
   ],
   "source": [
    "print(data_old[1]['neg'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giá trị có tần suất xuất hiện nhiều nhất là: 2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent(arr):\n",
    "    counter = Counter(arr)\n",
    "    most_common = counter.most_common(1)  # Lấy phần tử có tần suất xuất hiện cao nhất\n",
    "    return most_common[0][0]\n",
    "\n",
    "# Sử dụng hàm:\n",
    "array = [1, 2, 3, 4, 2, 2, 3, 1, 4, 2, 5, 5, 5, 5]\n",
    "print(\"Giá trị có tần suất xuất hiện nhiều nhất là:\", most_frequent(array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các giá trị tương ứng từ mảng n với các chỉ số từ mảng m: 50\n"
     ]
    }
   ],
   "source": [
    "def get_values_from_indices(array_n, array_m):\n",
    "    return [array_n[index] for index in array_m if index < len(array_n)]\n",
    "\n",
    "# Ví dụ sử dụng:\n",
    "array_n = [50, 20, 30, 40, 50, 60, 70]  # Mảng n phần tử\n",
    "array_m = [0, 2, 4]  # Mảng m phần tử (là chỉ số của mảng n)\n",
    "\n",
    "m_values = most_frequent(get_values_from_indices(array_n, array_m))\n",
    "print(\"Các giá trị tương ứng từ mảng n với các chỉ số từ mảng m:\", m_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giá trị có tần suất xuất hiện nhiều nhất: 2\n",
      "Chỉ số của giá trị có tần suất xuất hiện nhiều nhất: 1\n"
     ]
    }
   ],
   "source": [
    "def most_frequent_value(array):\n",
    "    \"\"\"\n",
    "    Trả về giá trị có tần suất xuất hiện nhiều nhất và một chỉ số của nó trong mảng.\n",
    "\n",
    "    Parameters:\n",
    "        array (list): Mảng chứa các giá trị.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Một tuple gồm giá trị có tần suất xuất hiện nhiều nhất và một chỉ số của nó.\n",
    "    \"\"\"\n",
    "    # Tạo một từ điển để đếm tần suất xuất hiện của từng giá trị\n",
    "    frequency_dict = {}\n",
    "    for index, value in enumerate(array):\n",
    "        if value in frequency_dict:\n",
    "            frequency_dict[value].append(index)\n",
    "        else:\n",
    "            frequency_dict[value] = [index]\n",
    "\n",
    "    # Tìm giá trị có tần suất xuất hiện cao nhất\n",
    "    max_frequency = max(len(indices) for indices in frequency_dict.values())\n",
    "    most_frequent_values = [value for value, indices in frequency_dict.items() if len(indices) == max_frequency]\n",
    "\n",
    "    # Chọn một chỉ số của giá trị có tần suất xuất hiện cao nhất\n",
    "    index_of_most_frequent_value = frequency_dict[most_frequent_values[0]][0]\n",
    "\n",
    "    return most_frequent_values[0], index_of_most_frequent_value\n",
    "\n",
    "# Ví dụ sử dụng:\n",
    "array = [1, 2, 3, 4, 2, 2, 3, 2, 5, 5, 5, 1, 2]\n",
    "most_frequent_val, index_of_most_frequent_val = most_frequent_value(array)\n",
    "print(\"Giá trị có tần suất xuất hiện nhiều nhất:\", most_frequent_val)\n",
    "print(\"Chỉ số của giá trị có tần suất xuất hiện nhiều nhất:\", index_of_most_frequent_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
