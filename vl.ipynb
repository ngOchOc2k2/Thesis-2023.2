{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Param\n",
    "\n",
    "param = Param()\n",
    "args = param.args\n",
    "args\n",
    "\n",
    "args.task_name = args.dataname\n",
    "\n",
    "# rel_per_task\n",
    "args.rel_per_task = 8 if args.dataname == \"FewRel\" else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu=0, dataname='FewRel', task_name='FewRel', device='cuda', batch_size=32, num_tasks=10, rel_per_task=8, pattern='entity_marker', max_length=128, encoder_output_size=768, vocab_size=30522, marker_size=4, num_workers=0, classifier_lr=0.01, encoder_lr=0.001, prompt_pool_lr=0.001, sgd_momentum=0.1, gmm_num_components=1, pull_constraint_coeff=0.1, classifier_epochs=10, encoder_epochs=10, prompt_pool_epochs=10, replay_s_e_e=256, replay_epochs=100, seed=2021, max_grad_norm=10, data_path='/home/luungoc/Thesis - 2023.2/datasets/', bert_path='bert-base-uncased', cov_mat=True, max_num_models=10, sample_freq=5, prompt_length=1, prompt_embed_dim=768, prompt_pool_size=80, prompt_top_k=8, prompt_init='uniform', prompt_key_init='uniform', drop_p=0.1, gradient_accumulation_steps=4, total_round=6, drop_out=0.5, use_gpu=True, rank=8, hidden_size=768)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import json, os\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np  \n",
    "\n",
    "\n",
    "def get_tokenizer(args):\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_path, additional_special_tokens=[\"[E11]\", \"[E12]\", \"[E21]\", \"[E22]\"])\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "class data_sampler(object):\n",
    "    def __init__(self, args, seed=None):\n",
    "        self.set_path(args)\n",
    "        self.args = args\n",
    "\n",
    "        # data path\n",
    "        file_name = \"{}.pkl\".format(\"-\".join([str(x) for x in [args.dataname, args.seed]]))\n",
    "        mid_dir = \"\"\n",
    "        for temp_p in [\"datasets\", \"_process_path\"]:\n",
    "            mid_dir = os.path.join(mid_dir, temp_p)\n",
    "            if not os.path.exists(mid_dir):\n",
    "                os.mkdir(mid_dir)\n",
    "        self.save_data_path = os.path.join(mid_dir, file_name)\n",
    "\n",
    "        # import tokenizer\n",
    "        self.tokenizer = get_tokenizer(args)\n",
    "\n",
    "        # read relation data\n",
    "        self.id2rel, self.rel2id = self._read_relations(args.relation_file)\n",
    "\n",
    "        # random sampling\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            random.seed(self.seed)\n",
    "        self.shuffle_index = list(range(len(self.id2rel)))\n",
    "        random.shuffle(self.shuffle_index)\n",
    "        self.shuffle_index = np.argsort(self.shuffle_index)\n",
    "\n",
    "        # regenerate data\n",
    "        self.training_dataset, self.valid_dataset, self.test_dataset = self._read_data(self.args.data_file)\n",
    "\n",
    "        # generate the task number\n",
    "        self.batch = 0\n",
    "        self.task_length = len(self.id2rel) // self.args.rel_per_task\n",
    "\n",
    "        # record relations\n",
    "        self.seen_relations = []\n",
    "        self.history_test_data = {}\n",
    "        \n",
    "        if args.dataname in [\"FewRel\"]:\n",
    "            self.id2rel = json.load(open(os.path.join(args.data_path, \"id2rel.json\"), 'r'))\n",
    "        else:\n",
    "            self.id2rel = json.load(open(os.path.join(args.data_path, \"id2rel_tacred.json\"), 'r'))\n",
    "        \n",
    "        self.rel2id = {label: idx for idx, label in enumerate(self.id2rel)}\n",
    "        \n",
    "\n",
    "    def set_path(self, args):\n",
    "        use_marker = \"\"\n",
    "        if args.dataname in [\"FewRel\"]:\n",
    "            args.data_file = os.path.join(args.data_path, \"data_with{}_marker.json\".format(use_marker))\n",
    "            args.relation_file = os.path.join(args.data_path, \"id2rel.json\")\n",
    "            args.num_of_relation = 80\n",
    "            args.num_of_train = 420\n",
    "            args.num_of_val = 140\n",
    "            args.num_of_test = 140\n",
    "            \n",
    "        elif args.dataname in [\"TACRED\"]:\n",
    "            args.data_file = os.path.join(args.data_path, \"data_with{}_marker_tacred.json\".format(use_marker))\n",
    "            args.relation_file = os.path.join(args.data_path, \"id2rel_tacred.json\")\n",
    "            args.num_of_relation = 40\n",
    "            args.num_of_train = 420\n",
    "            args.num_of_val = 140\n",
    "            args.num_of_test = 140\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        self.seed = seed\n",
    "        if self.seed != None:\n",
    "            random.seed(self.seed)\n",
    "        self.shuffle_index = list(range(len(self.id2rel)))\n",
    "        random.shuffle(self.shuffle_index)\n",
    "        self.shuffle_index = np.argsort(self.shuffle_index)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.batch == self.task_length:\n",
    "            raise StopIteration()\n",
    "\n",
    "        indexs = self.shuffle_index[self.args.rel_per_task * self.batch : self.args.rel_per_task * (self.batch + 1)]\n",
    "        self.batch += 1\n",
    "\n",
    "        current_relations = []\n",
    "        cur_training_data = {}\n",
    "        cur_valid_data = {}\n",
    "        cur_test_data = {}\n",
    "\n",
    "        for index in indexs:\n",
    "            current_relations.append(self.id2rel[index])\n",
    "            self.seen_relations.append(self.id2rel[index])\n",
    "            cur_training_data[self.id2rel[index]] = self.training_dataset[index]\n",
    "            cur_valid_data[self.id2rel[index]] = self.valid_dataset[index]\n",
    "            cur_test_data[self.id2rel[index]] = self.test_dataset[index]\n",
    "            self.history_test_data[self.id2rel[index]] = self.test_dataset[index]\n",
    "\n",
    "        return cur_training_data, cur_valid_data, cur_test_data, current_relations, self.history_test_data, self.seen_relations\n",
    "\n",
    "    def _read_data(self, file):\n",
    "        if os.path.isfile(self.save_data_path):\n",
    "            with open(self.save_data_path, \"rb\") as f:\n",
    "                datas = pickle.load(f)\n",
    "            train_dataset, val_dataset, test_dataset = datas\n",
    "            return train_dataset, val_dataset, test_dataset\n",
    "        else:\n",
    "            data = json.load(open(file, \"r\", encoding=\"utf-8\"))\n",
    "            train_dataset = [[] for i in range(self.args.num_of_relation)]\n",
    "            val_dataset = [[] for i in range(self.args.num_of_relation)]\n",
    "            test_dataset = [[] for i in range(self.args.num_of_relation)]\n",
    "            for relation in data.keys():\n",
    "                rel_samples = data[relation]\n",
    "                if self.seed != None:\n",
    "                    random.seed(self.seed)\n",
    "                random.shuffle(rel_samples)\n",
    "                count = 0\n",
    "                count1 = 0\n",
    "                for i, sample in enumerate(rel_samples):\n",
    "                    tokenized_sample = {}\n",
    "                    tokenized_sample[\"relation\"] = self.rel2id[sample[\"relation\"]]\n",
    "                    tokenized_sample[\"text\"] = \" \".join(sample[\"tokens\"])\n",
    "                    tokenized_sample[\"tokens\"] = self.tokenizer.encode(\" \".join(sample[\"tokens\"]), padding=\"max_length\", truncation=True, max_length=self.args.max_length)\n",
    "\n",
    "\n",
    "                    if self.args.task_name == \"FewRel\":\n",
    "                        if i < self.args.num_of_train:\n",
    "                            train_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                        elif i < self.args.num_of_train + self.args.num_of_val:\n",
    "                            val_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                        else:\n",
    "                            test_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                    else:\n",
    "                        if i < len(rel_samples) // 5 and count <= 40:\n",
    "                            count += 1\n",
    "                            test_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                        else:\n",
    "                            count1 += 1\n",
    "                            train_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                            if count1 >= 320:\n",
    "                                break\n",
    "\n",
    "                    \n",
    "            with open(self.save_data_path, \"wb\") as f:\n",
    "                pickle.dump((train_dataset, val_dataset, test_dataset), f)\n",
    "            return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def _read_relations(self, file):\n",
    "        id2rel = json.load(open(file, \"r\", encoding=\"utf-8\"))\n",
    "        rel2id = {}\n",
    "        for i, x in enumerate(id2rel):\n",
    "            rel2id[x] = i\n",
    "        return id2rel, rel2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(args.seed)\n",
    "print(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relation': 26, 'text': 'She is noted for playing a vintage Rhodes Piano on both Denali albums , their [E11] self - titled album [E12] and its follow - up , [E21] The Instinct [E22] .', 'tokens': [101, 2016, 2003, 3264, 2005, 2652, 1037, 13528, 10588, 3682, 2006, 2119, 7939, 11475, 4042, 1010, 2037, 30522, 2969, 1011, 4159, 2201, 30523, 1998, 2049, 3582, 1011, 2039, 1010, 30524, 1996, 12753, 30525, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 72, 'text': '[E11] Renata Pokupić [E12] ( born July 24 , 1972 , in Virovitica , Republic of Croatia ) is a Croatian operatic [E21] mezzo - soprano [E22] .', 'tokens': [101, 30522, 14916, 6790, 13433, 5283, 24330, 30523, 1006, 2141, 2251, 2484, 1010, 3285, 1010, 1999, 6819, 12298, 18291, 2050, 1010, 3072, 1997, 8097, 1007, 2003, 1037, 7963, 22534, 30524, 2033, 12036, 1011, 10430, 30525, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 5, 'text': '[E21] Jan Stráský [E22] ( born 24 December 1940 in Plzeň ) is a Czech politician , who notably served as the last Prime Minister of [E11] Czechoslovakia [E12] in 1992 .', 'tokens': [101, 30524, 5553, 2358, 8180, 4801, 30525, 1006, 2141, 2484, 2285, 3878, 1999, 20228, 10431, 1007, 2003, 1037, 5569, 3761, 1010, 2040, 5546, 2366, 2004, 1996, 2197, 3539, 2704, 1997, 30522, 12833, 30523, 1999, 2826, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 7, 'text': 'His first major role was as Ed the Bodyguard in the [E21] Family Channel [E22] series [E11] Connor Undercover [E12] .', 'tokens': [101, 2010, 2034, 2350, 2535, 2001, 2004, 3968, 1996, 16174, 1999, 1996, 30524, 2155, 3149, 30525, 2186, 30522, 6720, 16382, 30523, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 14, 'text': '[E11] Edwards Dam [E12] was a hydroelectric dam on the [E21] Kennebec River [E22] in the U.S. state of Maine .', 'tokens': [101, 30522, 7380, 5477, 30523, 2001, 1037, 18541, 5477, 2006, 1996, 30524, 6358, 2638, 4783, 2278, 2314, 30525, 1999, 1996, 1057, 1012, 1055, 1012, 2110, 1997, 7081, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 71, 'text': '[E11] Daugavgrīva Lighthouse [E12] ( Latvian : \" Daugavgrīvas bāka \" ) is a lighthouse located in Daugavgrīva on the [E21] Bay of Riga [E22] on the Latvian coast of the Baltic Sea .', 'tokens': [101, 30522, 4830, 16377, 2615, 16523, 11444, 10171, 30523, 1006, 14698, 1024, 1000, 4830, 16377, 2615, 16523, 11444, 2015, 8670, 2912, 1000, 1007, 2003, 1037, 10171, 2284, 1999, 4830, 16377, 2615, 16523, 11444, 2006, 1996, 30524, 3016, 1997, 17557, 30525, 2006, 1996, 14698, 3023, 1997, 1996, 11275, 2712, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 33, 'text': 'Her mother was the daughter of sebastocrator [E21] Constantine Palaiologos [E22] and niece of [E11] Michael VIII Palaiologos [E12] .', 'tokens': [101, 2014, 2388, 2001, 1996, 2684, 1997, 7367, 22083, 3406, 23185, 2953, 30524, 12790, 14412, 4886, 12898, 12333, 30525, 1998, 12286, 1997, 30522, 2745, 9937, 14412, 4886, 12898, 12333, 30523, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 74, 'text': 'Godfrey was a native of Leuven and was chaplain to [E11] Adeliza of Louvain [E12] , second wife of King [E21] Henry I of England [E22] .', 'tokens': [101, 18238, 2001, 1037, 3128, 1997, 3393, 27346, 1998, 2001, 14011, 2000, 30522, 4748, 20806, 4143, 1997, 10223, 3567, 2378, 30523, 1010, 2117, 2564, 1997, 2332, 30524, 2888, 1045, 1997, 2563, 30525, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 22, 'text': 'miles , from the northern coastal area of [E21] Shark Bay [E22] between the Gascoyne and [E11] Wooramel [E12] rivers .', 'tokens': [101, 2661, 1010, 2013, 1996, 2642, 5780, 2181, 1997, 30524, 11420, 3016, 30525, 2090, 1996, 3806, 3597, 9654, 1998, 30522, 15854, 6444, 2884, 30523, 5485, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 27, 'text': \"Overall [E21] Alfa Romeo [E22] 's 3 litre formula cars ( Tipo 308 , [E11] 312 [E12] and 316 ) were not a great success .\", 'tokens': [101, 3452, 30524, 22989, 12390, 30525, 1005, 1055, 1017, 16812, 5675, 3765, 1006, 5955, 2080, 24232, 1010, 30522, 21036, 30523, 1998, 23980, 1007, 2020, 2025, 1037, 2307, 3112, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "data = data_sampler(args, seed=2021)\n",
    "list_data = []\n",
    "\n",
    "for steps, (training_data, valid_data, test_data, current_relations, historic_test_data, seen_relations) in enumerate(data):\n",
    "    \n",
    "    # task_x = []\n",
    "    # print(current_relations)\n",
    "    # for relation in current_relations:\n",
    "    #     for sample in test_data[relation]:\n",
    "    #         task_x.append({\n",
    "    #             'text': sample['text'],\n",
    "    #             'relation': sample['relation'],\n",
    "    #             'description': new_pt,\n",
    "    #             'chunk': current_relations,\n",
    "    #         })\n",
    "    # list_data.append(task_x)\n",
    "    \n",
    "    print(training_data[current_relations[0]][0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data_ds = json.load(open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/format/prompt.json', 'r'))\n",
    "data2 = json.load(open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/format/prompt1.json', 'r'))\n",
    "\n",
    "data_ds += data2\n",
    "len(data_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(lis <sdghk'\n",
    "          23, open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/datasets/retrieval.json', 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relations described above serve to categorize and clarify the connections or associations between different entities within text data. Understanding these relations helps in organizing information more effectively, particularly in fields like knowledge graph construction, information retrieval, and natural language processing tasks. Here's an explanation of the sense of each relation:\n",
      "\n",
      "1. **org:shareholders** - This relation identifies a party (individual or group) that owns shares in a corporation or organization, indicating a financial interest or partial ownership. The relation clarifies who has a stake in the company, which can affect decisions, control, or influence within the organization.\n",
      "\n",
      "2. **org:parents** - This relation defines the ownership or control relationship between two corporations, specifically pointing out that one entity (the parent) owns another entity (the subsidiary). This can influence the operations, policies, and strategic direction of the subsidiary and shows the hierarchical structure within or between corporate entities.\n",
      "\n",
      "3. **org:city_of_headquarters** - This relation links an organization or company with the city where its main office or headquarters is located. Knowing an organization’s headquarters can provide insights into its geographic footprint, legal jurisdiction, and possibly, the cultural or regional influences on its operations.\n",
      "\n",
      "4. **per:religion** - This relation connects an individual with their religious affiliation or beliefs. Understanding an individual's religion can add context to their motivations, cultural background, and personal values. This relation is significant in biographical data, sociological research, and cultural studies.\n",
      "\n",
      "These relations serve as crucial components in structuring data about entities and their interconnections. By categorizing these relationships, it becomes easier to navigate complex information and derive meaningful insights.\n"
     ]
    }
   ],
   "source": [
    "print(data[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = json.load(open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/format/format.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = \"You are a useful information extraction machine. Read the examples carefully and explain the sense of five relations above (note: not analysis the examples).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pt = []\n",
    "\n",
    "for it1, it2 in zip(format, data_ds):\n",
    "    new_pt.append(it1.replace(temp, it2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User information\n",
      "----------------\n",
      "Example 1: \n",
      "{\n",
      "    'context': `` [E21] ontario [E22] is taking the next step towards recovering taxpayer dollars spent fighting tobacco-related illnesses , '' ontario attorney general [E11] chris bentley [E12] said in a statement .,\n",
      "    'entity_1': chris bentley,\n",
      "    'entity_2': ontario,\n",
      "    'relation': per:stateorprovinces_of_residence\n",
      "}\n",
      "\n",
      "Example 2: \n",
      "{\n",
      "    'context': beijing , dec 27 -lrb- xinhua -rrb- the original china southwest airlines and [E11] china national aviation corporation [E12] -lrb- [E21] cnac [E22] -rrb- will use the same airline code and numeric code as air china from january 1 , 2003 .,\n",
      "    'entity_1': china national aviation corporation,\n",
      "    'entity_2': cnac,\n",
      "    'relation': org:alternate_names\n",
      "}\n",
      "\n",
      "Example 3: \n",
      "{\n",
      "    'context': the recent merger announced between the [E11] american bankers association [E12] and [E21] america [E22] 's community bankers has done more than create the nation 's largest banking trade association .,\n",
      "    'entity_1': american bankers association,\n",
      "    'entity_2': america,\n",
      "    'relation': org:country_of_headquarters\n",
      "}\n",
      "\n",
      "Example 4: \n",
      "{\n",
      "    'context': [E11] theodor kollek [E12] was born may 27 , 1911 , in the hungarian town of nagyvazsony and raised in vienna , [E21] austria [E22] .,\n",
      "    'entity_1': theodor kollek,\n",
      "    'entity_2': austria,\n",
      "    'relation': per:country_of_birth\n",
      "}\n",
      "\n",
      "----------------\n",
      "Relation\n",
      "----------------\n",
      "[\n",
      "    per:stateorprovinces_of_residence,\n",
      "    org:alternate_names,\n",
      "    org:country_of_headquarters,\n",
      "    per:country_of_birth\n",
      "]\n",
      "----------------\n",
      "The examples provided serve to illustrate how specific relationships or \"relations\" between entities (people, organizations, places, etc.) can be extracted from textual data. Let's explore the meaning of the four relations mentioned:\n",
      "\n",
      "1. **per:stateorprovinces_of_residence**: This relation identifies the state or province in which a person (indicated by \"per\" for a personal entity) resides or has residence. The key aspect of this relationship is to pinpoint a geographic area within a larger national context where the individual lives or spends a significant amount of their time.\n",
      "\n",
      "2. **org:alternate_names**: This relation highlights that an organization (indicated by \"org\" for an organizational entity) is known by more than one name or has alternate names/titles. This could include acronyms, abbreviations, former names, or any other form of official or unofficial title that refers to the same entity. The relationship is crucial for recognizing and connecting different identifiers that may be used in various contexts to refer to the same organization.\n",
      "\n",
      "3. **org:country_of_headquarters**: This relation specifies the country in which an organization's headquarters is located. It reveals a geographical and national context for the organization's primary base of operations. This information is essential for understanding the legal and cultural jurisdiction that the organization primarily operates within and identifies from a global perspective.\n",
      "\n",
      "4. **per:country_of_birth**: This relation indicates the country where a person (again, \"per\" for a personal entity) was born. It directly points to the nationality or original geographic identity of the individual at the time of birth, possibly influencing their citizenship, cultural identity, and legal rights in some contexts.\n",
      "\n",
      "Each of these relations serves to extract and clarify specific types of connections and contexts that are inherently valuable for various applications, including data analysis, knowledge graphs, and deepening the understanding of textual information content.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(new_pt[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Lấy đường dẫn của tập tin hiện tại\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m current_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mĐường dẫn của tập tin hiện tại:\u001b[39m\u001b[38;5;124m\"\u001b[39m, current_file_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Lấy đường dẫn của tập tin hiện tại\n",
    "current_file_path = os.path.abspath(__file__)\n",
    "\n",
    "print(\"Đường dẫn của tập tin hiện tại:\", current_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
