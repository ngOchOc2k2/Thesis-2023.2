{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Param\n",
    "\n",
    "param = Param()\n",
    "args = param.args\n",
    "args\n",
    "\n",
    "args.task_name = args.dataname\n",
    "\n",
    "# rel_per_task\n",
    "args.rel_per_task = 8 if args.dataname == \"FewRel\" else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu=0, dataname='TACRED', task_name='TACRED', device='cuda', batch_size=64, num_tasks=10, rel_per_task=4, pattern='entity_marker', max_length=192, encoder_output_size=768, vocab_size=30522, marker_size=4, num_workers=0, save_checkpoint='./checkpoint/', classifier_lr=0.01, encoder_lr=0.001, prompt_pool_lr=0.001, sgd_momentum=0.1, gmm_num_components=1, pull_constraint_coeff=0.1, classifier_epochs=10, encoder_epochs=10, prompt_pool_epochs=10, replay_s_e_e=256, replay_epochs=100, seed=2021, max_grad_norm=10, data_path='./datasets', bert_path='bert-base-uncased', cov_mat=True, max_num_models=10, sample_freq=5, prompt_length=1, prompt_embed_dim=768, prompt_pool_size=80, prompt_top_k=8, prompt_init='uniform', prompt_key_init='uniform', drop_p=0.1, gradient_accumulation_steps=4, total_round=6, drop_out=0.5, use_gpu=True, hidden_size=768, rank_lora=8, bge_model='BAAI/bge-m3', description_path='/kaggle/input/data-relation/datasets/description/ngoc.json', type_similar='colbert')\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import json, os\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np  \n",
    "\n",
    "\n",
    "def get_tokenizer(args):\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_path, additional_special_tokens=[\"[E11]\", \"[E12]\", \"[E21]\", \"[E22]\"])\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "class data_sampler(object):\n",
    "    def __init__(self, args, seed=None):\n",
    "        self.set_path(args)\n",
    "        self.args = args\n",
    "\n",
    "        # data path\n",
    "        file_name = \"{}.pkl\".format(\"-\".join([str(x) for x in [args.dataname, args.seed]]))\n",
    "        mid_dir = \"\"\n",
    "        for temp_p in [\"datasets\", \"_process_path\"]:\n",
    "            mid_dir = os.path.join(mid_dir, temp_p)\n",
    "            if not os.path.exists(mid_dir):\n",
    "                os.mkdir(mid_dir)\n",
    "        self.save_data_path = os.path.join(mid_dir, file_name)\n",
    "\n",
    "        # import tokenizer\n",
    "        self.tokenizer = get_tokenizer(args)\n",
    "\n",
    "        # read relation data\n",
    "        self.id2rel, self.rel2id = self._read_relations(args.relation_file)\n",
    "\n",
    "        # random sampling\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            random.seed(self.seed)\n",
    "        self.shuffle_index = list(range(len(self.id2rel)))\n",
    "        random.shuffle(self.shuffle_index)\n",
    "        self.shuffle_index = np.argsort(self.shuffle_index)\n",
    "\n",
    "        # regenerate data\n",
    "        self.training_dataset, self.valid_dataset, self.test_dataset = self._read_data(self.args.data_file)\n",
    "\n",
    "        # generate the task number\n",
    "        self.batch = 0\n",
    "        self.task_length = len(self.id2rel) // self.args.rel_per_task\n",
    "\n",
    "        # record relations\n",
    "        self.seen_relations = []\n",
    "        self.history_test_data = {}\n",
    "        \n",
    "        if args.dataname in [\"FewRel\"]:\n",
    "            self.id2rel = json.load(open(os.path.join(args.data_path, \"id2rel.json\"), 'r'))\n",
    "        else:\n",
    "            self.id2rel = json.load(open(os.path.join(args.data_path, \"id2rel_tacred.json\"), 'r'))\n",
    "        \n",
    "        self.rel2id = {label: idx for idx, label in enumerate(self.id2rel)}\n",
    "        \n",
    "\n",
    "    def set_path(self, args):\n",
    "        use_marker = \"\"\n",
    "        if args.dataname in [\"FewRel\"]:\n",
    "            args.data_file = os.path.join(args.data_path, \"data_with{}_marker.json\".format(use_marker))\n",
    "            args.relation_file = os.path.join(args.data_path, \"id2rel.json\")\n",
    "            args.num_of_relation = 80\n",
    "            args.num_of_train = 420\n",
    "            args.num_of_val = 140\n",
    "            args.num_of_test = 140\n",
    "            \n",
    "        elif args.dataname in [\"TACRED\"]:\n",
    "            args.data_file = os.path.join(args.data_path, \"data_with{}_marker_tacred.json\".format(use_marker))\n",
    "            args.relation_file = os.path.join(args.data_path, \"id2rel_tacred.json\")\n",
    "            args.num_of_relation = 40\n",
    "            args.num_of_train = 420\n",
    "            args.num_of_val = 140\n",
    "            args.num_of_test = 140\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        self.seed = seed\n",
    "        if self.seed != None:\n",
    "            random.seed(self.seed)\n",
    "        self.shuffle_index = list(range(len(self.id2rel)))\n",
    "        random.shuffle(self.shuffle_index)\n",
    "        self.shuffle_index = np.argsort(self.shuffle_index)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.batch == self.task_length:\n",
    "            raise StopIteration()\n",
    "\n",
    "        indexs = self.shuffle_index[self.args.rel_per_task * self.batch : self.args.rel_per_task * (self.batch + 1)]\n",
    "        self.batch += 1\n",
    "\n",
    "        current_relations = []\n",
    "        cur_training_data = {}\n",
    "        cur_valid_data = {}\n",
    "        cur_test_data = {}\n",
    "\n",
    "        for index in indexs:\n",
    "            current_relations.append(self.id2rel[index])\n",
    "            self.seen_relations.append(self.id2rel[index])\n",
    "            cur_training_data[self.id2rel[index]] = self.training_dataset[index]\n",
    "            cur_valid_data[self.id2rel[index]] = self.valid_dataset[index]\n",
    "            cur_test_data[self.id2rel[index]] = self.test_dataset[index]\n",
    "            self.history_test_data[self.id2rel[index]] = self.test_dataset[index]\n",
    "\n",
    "        return cur_training_data, cur_valid_data, cur_test_data, current_relations, self.history_test_data, self.seen_relations\n",
    "\n",
    "    def _read_data(self, file):\n",
    "        if os.path.isfile(self.save_data_path):\n",
    "            with open(self.save_data_path, \"rb\") as f:\n",
    "                datas = pickle.load(f)\n",
    "            train_dataset, val_dataset, test_dataset = datas\n",
    "            return train_dataset, val_dataset, test_dataset\n",
    "        else:\n",
    "            data = json.load(open(file, \"r\", encoding=\"utf-8\"))\n",
    "            train_dataset = [[] for i in range(self.args.num_of_relation)]\n",
    "            val_dataset = [[] for i in range(self.args.num_of_relation)]\n",
    "            test_dataset = [[] for i in range(self.args.num_of_relation)]\n",
    "            for relation in data.keys():\n",
    "                rel_samples = data[relation]\n",
    "                if self.seed != None:\n",
    "                    random.seed(self.seed)\n",
    "                random.shuffle(rel_samples)\n",
    "                count = 0\n",
    "                count1 = 0\n",
    "                for i, sample in enumerate(rel_samples):\n",
    "                    tokenized_sample = {}\n",
    "                    tokenized_sample[\"relation\"] = self.rel2id[sample[\"relation\"]]\n",
    "                    tokenized_sample[\"text\"] = \" \".join(sample[\"tokens\"])\n",
    "                    tokenized_sample[\"tokens\"] = self.tokenizer.encode(\" \".join(sample[\"tokens\"]), padding=\"max_length\", truncation=True, max_length=self.args.max_length)\n",
    "\n",
    "\n",
    "                    if self.args.task_name == \"FewRel\":\n",
    "                        if i < self.args.num_of_train:\n",
    "                            train_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                        elif i < self.args.num_of_train + self.args.num_of_val:\n",
    "                            val_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                        else:\n",
    "                            test_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                    else:\n",
    "                        if i < len(rel_samples) // 5 and count <= 40:\n",
    "                            count += 1\n",
    "                            test_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                        else:\n",
    "                            count1 += 1\n",
    "                            train_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                            if count1 >= 320:\n",
    "                                break\n",
    "\n",
    "                    \n",
    "            with open(self.save_data_path, \"wb\") as f:\n",
    "                pickle.dump((train_dataset, val_dataset, test_dataset), f)\n",
    "            return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def _read_relations(self, file):\n",
    "        id2rel = json.load(open(file, \"r\", encoding=\"utf-8\"))\n",
    "        rel2id = {}\n",
    "        for i, x in enumerate(id2rel):\n",
    "            rel2id[x] = i\n",
    "        return id2rel, rel2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = \"/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/datasets/data_with_marker_tacred.json\"\n",
    "data = json.load(open(file, \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(args.seed)\n",
    "print(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['per:cities_of_residence', 'per:other_family', 'org:founded', 'per:origin']\n",
      "['per:cause_of_death', 'org:dissolved', 'per:employee_of', 'org:member_of']\n",
      "['per:parents', 'per:alternate_names', 'org:top_members/employees', 'per:siblings']\n",
      "['per:stateorprovinces_of_residence', 'org:alternate_names', 'org:country_of_headquarters', 'per:country_of_birth']\n",
      "['per:children', 'per:date_of_birth', 'org:founded_by', 'per:countries_of_residence']\n",
      "['per:schools_attended', 'org:subsidiaries', 'org:members', 'org:political/religious_affiliation']\n",
      "['org:stateorprovince_of_headquarters', 'per:charges', 'per:stateorprovince_of_birth', 'per:title']\n",
      "['per:stateorprovince_of_death', 'org:number_of_employees/members', 'per:city_of_death', 'per:spouse']\n",
      "['org:website', 'per:age', 'per:city_of_birth', 'per:date_of_death']\n",
      "['org:shareholders', 'org:parents', 'org:city_of_headquarters', 'per:religion']\n"
     ]
    }
   ],
   "source": [
    "data = data_sampler(args, seed=2021)\n",
    "list_data = []\n",
    "\n",
    "for steps, (training_data, valid_data, test_data, current_relations, historic_test_data, seen_relations) in enumerate(data):\n",
    "    \n",
    "    task_x = []\n",
    "    for relation in current_relations:\n",
    "        list_data.append({\n",
    "            'relation': relation,\n",
    "            'text': [item['text'] for item in training_data[relation][5:10]]\n",
    "        })\n",
    "    # list_data.append(task_x)\n",
    "    \n",
    "        # for item in training_data[relation]:\n",
    "        #     if item['relation'] == 0:\n",
    "        #         print(item)\n",
    "    # if training_data[current_relations[0]][21]['relation'] == 0:\n",
    "    # print(training_data[current_relations[0]][21])\n",
    "    print(current_relations)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_data)\n",
    "\n",
    "json.dump(list_data, open('./prototype.json', 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "x = json.load(open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/datasets/description/all.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The relation name \"org:stateorprovince_of_headquarters\" refers to the specific type of relationship between an organization (E11, E12) and the state or province where that organization's headquarters are located (E21, E22). This relationship is identified from textual data that explicitly or implicitly mentions the geographical location of the main office or central operating facility of an organization. The examples provided illustrate various instances where the headquarters of an organization are linked to a particular state or province, showcasing the diversity in organizational types and the geographical spread of their central operations.\n",
    "\n",
    "The \"org:stateorprovince_of_headquarters\" relation is pivotal for understanding the geographic distribution of organizational headquarters, which can offer insights into the regional economic impact, employment patterns, and the strategic decisions behind where companies choose to base their main operations. This relationship helps in mapping out the corporate landscape of different regions, indicating the concentration of industries, the preference of organizations for certain locales due to logistical, economic, or regulatory advantages, and the potential influence of these organizations on local economies and policies.\n",
    "\n",
    "Understanding this relation is crucial for economic analysis, regional planning, and the study of corporate strategies. It aids in identifying how organizations are distributed across states or provinces, which can be essential for supply chain logistics, marketing strategies, and understanding the regional dynamics of business operations. This relation also has implications for tax policies, infrastructure development, and workforce distribution, offering a lens through which to view the interactions between corporations and the geographical contexts of their headquarters.\n",
    "\"\"\"\n",
    "for item in x:\n",
    "    if item['relation'] == \"org:stateorprovince_of_headquarters\":\n",
    "        print(\"Yes\")\n",
    "        item['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được lưu vào file JSONL: ./train_retrieval.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_to_jsonl(data, file_path):\n",
    "    \"\"\"\n",
    "    Lưu dữ liệu vào một file JSONL.\n",
    "\n",
    "    Parameters:\n",
    "        data (list): Danh sách các đối tượng để lưu vào file JSONL.\n",
    "        file_path (str): Đường dẫn tới file JSONL.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\") as jsonl_file:\n",
    "        for item in data:\n",
    "            json.dump(item, jsonl_file)  # Ghi một đối tượng JSON vào file\n",
    "            jsonl_file.write(\"\\n\")  # Viết dấu xuống dòng sau mỗi đối tượng\n",
    "\n",
    "    print(\"Dữ liệu đã được lưu vào file JSONL:\", file_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "a = json.load(open('/home/luungoc/Thesis - 2023.2/train_bge.json', 'r'))\n",
    "\n",
    "\n",
    "for item in a:\n",
    "    item['neg'] = []\n",
    "    \n",
    "    \n",
    "save_to_jsonl(a, './train_retrieval.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'The relation \"per:cities_of_residence\" identifies and connects individuals (E11-E12) with cities or locations (E21-E22) where they reside or have resided. This relation is crucial for understanding geographical ties or the significance of certain locations in a person\\'s life. Analyzing examples provided, we can observe the nature and application of this relation in various contexts.\\nThis relation maps individuals to geographical locations where they live or have a significant presence. It\\'s not limited to cities but can include towns, regions, or even specific areas like islands or rivers if they\\'re central to the individual\\'s life or activities. The relation is critical for understanding personal, social, and professional ties to specific places, contributing to a comprehensive view of a person\\'s life and actions. It can be explicitly stated, as in an individual being mentioned as living in a place, or implied through significant involvement or influence in the area.\\n',\n",
       " 'pos': ['over the next four decades , [E11] washburn [E12] reshaped the museum , shepherding it from its back bay location to a new site -- under a new name , the museum of science -- along the [E21] charles river [E22] on the border of boston and cambridge , mass. , and increased the size of its staff substantially .']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
