{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Param\n",
    "\n",
    "param = Param()\n",
    "args = param.args\n",
    "args\n",
    "\n",
    "args.task_name = args.dataname\n",
    "\n",
    "# rel_per_task\n",
    "args.rel_per_task = 8 if args.dataname == \"FewRel\" else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu=0, dataname='TACRED', task_name='TACRED', device='cuda', batch_size=64, num_tasks=10, rel_per_task=4, pattern='entity_marker', max_length=192, encoder_output_size=768, vocab_size=30522, marker_size=4, num_workers=0, save_checkpoint='./checkpoint/', classifier_lr=0.01, encoder_lr=0.001, prompt_pool_lr=0.001, sgd_momentum=0.1, gmm_num_components=1, pull_constraint_coeff=0.1, classifier_epochs=10, encoder_epochs=10, prompt_pool_epochs=10, replay_s_e_e=256, replay_epochs=100, seed=2021, max_grad_norm=10, data_path='./datasets', bert_path='bert-base-uncased', cov_mat=True, max_num_models=10, sample_freq=5, prompt_length=1, prompt_embed_dim=768, prompt_pool_size=80, prompt_top_k=8, prompt_init='uniform', prompt_key_init='uniform', drop_p=0.1, gradient_accumulation_steps=4, total_round=6, drop_out=0.5, use_gpu=True, hidden_size=768, rank_lora=8, bge_model='BAAI/bge-m3', description_path='./description/all-2.json', type_similar='colbert')\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import json, os\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np  \n",
    "\n",
    "\n",
    "def get_tokenizer(args):\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_path, additional_special_tokens=[\"[E11]\", \"[E12]\", \"[E21]\", \"[E22]\"])\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "class data_sampler(object):\n",
    "    def __init__(self, args, seed=None):\n",
    "        self.set_path(args)\n",
    "        self.args = args\n",
    "\n",
    "        # data path\n",
    "        file_name = \"{}.pkl\".format(\"-\".join([str(x) for x in [args.dataname, args.seed]]))\n",
    "        mid_dir = \"\"\n",
    "        for temp_p in [\"datasets\", \"_process_path\"]:\n",
    "            mid_dir = os.path.join(mid_dir, temp_p)\n",
    "            if not os.path.exists(mid_dir):\n",
    "                os.mkdir(mid_dir)\n",
    "        self.save_data_path = os.path.join(mid_dir, file_name)\n",
    "\n",
    "        # import tokenizer\n",
    "        self.tokenizer = get_tokenizer(args)\n",
    "\n",
    "        # read relation data\n",
    "        self.id2rel, self.rel2id = self._read_relations(args.relation_file)\n",
    "\n",
    "        # random sampling\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            random.seed(self.seed)\n",
    "        self.shuffle_index = list(range(len(self.id2rel)))\n",
    "        random.shuffle(self.shuffle_index)\n",
    "        self.shuffle_index = np.argsort(self.shuffle_index)\n",
    "\n",
    "        # regenerate data\n",
    "        self.training_dataset, self.valid_dataset, self.test_dataset = self._read_data(self.args.data_file)\n",
    "\n",
    "        # generate the task number\n",
    "        self.batch = 0\n",
    "        self.task_length = len(self.id2rel) // self.args.rel_per_task\n",
    "\n",
    "        # record relations\n",
    "        self.seen_relations = []\n",
    "        self.history_test_data = {}\n",
    "        \n",
    "        if args.dataname in [\"FewRel\"]:\n",
    "            self.id2rel = json.load(open(os.path.join(args.data_path, \"id2rel.json\"), 'r'))\n",
    "        else:\n",
    "            self.id2rel = json.load(open(os.path.join(args.data_path, \"id2rel_tacred.json\"), 'r'))\n",
    "        \n",
    "        self.rel2id = {label: idx for idx, label in enumerate(self.id2rel)}\n",
    "        \n",
    "\n",
    "    def set_path(self, args):\n",
    "        use_marker = \"\"\n",
    "        if args.dataname in [\"FewRel\"]:\n",
    "            args.data_file = os.path.join(args.data_path, \"data_with{}_marker.json\".format(use_marker))\n",
    "            args.relation_file = os.path.join(args.data_path, \"id2rel.json\")\n",
    "            args.num_of_relation = 80\n",
    "            args.num_of_train = 420\n",
    "            args.num_of_val = 140\n",
    "            args.num_of_test = 140\n",
    "            \n",
    "        elif args.dataname in [\"TACRED\"]:\n",
    "            args.data_file = os.path.join(args.data_path, \"data_with{}_marker_tacred.json\".format(use_marker))\n",
    "            args.relation_file = os.path.join(args.data_path, \"id2rel_tacred.json\")\n",
    "            args.num_of_relation = 40\n",
    "            args.num_of_train = 420\n",
    "            args.num_of_val = 140\n",
    "            args.num_of_test = 140\n",
    "\n",
    "    def set_seed(self, seed):\n",
    "        self.seed = seed\n",
    "        if self.seed != None:\n",
    "            random.seed(self.seed)\n",
    "        self.shuffle_index = list(range(len(self.id2rel)))\n",
    "        random.shuffle(self.shuffle_index)\n",
    "        self.shuffle_index = np.argsort(self.shuffle_index)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.batch == self.task_length:\n",
    "            raise StopIteration()\n",
    "\n",
    "        indexs = self.shuffle_index[self.args.rel_per_task * self.batch : self.args.rel_per_task * (self.batch + 1)]\n",
    "        self.batch += 1\n",
    "\n",
    "        current_relations = []\n",
    "        cur_training_data = {}\n",
    "        cur_valid_data = {}\n",
    "        cur_test_data = {}\n",
    "\n",
    "        for index in indexs:\n",
    "            current_relations.append(self.id2rel[index])\n",
    "            self.seen_relations.append(self.id2rel[index])\n",
    "            cur_training_data[self.id2rel[index]] = self.training_dataset[index]\n",
    "            cur_valid_data[self.id2rel[index]] = self.valid_dataset[index]\n",
    "            cur_test_data[self.id2rel[index]] = self.test_dataset[index]\n",
    "            self.history_test_data[self.id2rel[index]] = self.test_dataset[index]\n",
    "\n",
    "        return cur_training_data, cur_valid_data, cur_test_data, current_relations, self.history_test_data, self.seen_relations\n",
    "\n",
    "    def _read_data(self, file):\n",
    "        if os.path.isfile(self.save_data_path):\n",
    "            with open(self.save_data_path, \"rb\") as f:\n",
    "                datas = pickle.load(f)\n",
    "            train_dataset, val_dataset, test_dataset = datas\n",
    "            return train_dataset, val_dataset, test_dataset\n",
    "        else:\n",
    "            data = json.load(open(file, \"r\", encoding=\"utf-8\"))\n",
    "            train_dataset = [[] for i in range(self.args.num_of_relation)]\n",
    "            val_dataset = [[] for i in range(self.args.num_of_relation)]\n",
    "            test_dataset = [[] for i in range(self.args.num_of_relation)]\n",
    "            for relation in data.keys():\n",
    "                rel_samples = data[relation]\n",
    "                if self.seed != None:\n",
    "                    random.seed(self.seed)\n",
    "                random.shuffle(rel_samples)\n",
    "                count = 0\n",
    "                count1 = 0\n",
    "                for i, sample in enumerate(rel_samples):\n",
    "                    tokenized_sample = {}\n",
    "                    tokenized_sample[\"relation\"] = self.rel2id[sample[\"relation\"]]\n",
    "                    tokenized_sample[\"text\"] = \" \".join(sample[\"tokens\"])\n",
    "                    tokenized_sample[\"tokens\"] = self.tokenizer.encode(\" \".join(sample[\"tokens\"]), padding=\"max_length\", truncation=True, max_length=self.args.max_length)\n",
    "\n",
    "\n",
    "                    if self.args.task_name == \"FewRel\":\n",
    "                        if i < self.args.num_of_train:\n",
    "                            train_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                        elif i < self.args.num_of_train + self.args.num_of_val:\n",
    "                            val_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                        else:\n",
    "                            test_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                    else:\n",
    "                        if i < len(rel_samples) // 5 and count <= 40:\n",
    "                            count += 1\n",
    "                            test_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                        else:\n",
    "                            count1 += 1\n",
    "                            train_dataset[self.rel2id[relation]].append(tokenized_sample)\n",
    "                            if count1 >= 320:\n",
    "                                break\n",
    "\n",
    "                    \n",
    "            with open(self.save_data_path, \"wb\") as f:\n",
    "                pickle.dump((train_dataset, val_dataset, test_dataset), f)\n",
    "            return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def _read_relations(self, file):\n",
    "        id2rel = json.load(open(file, \"r\", encoding=\"utf-8\"))\n",
    "        rel2id = {}\n",
    "        for i, x in enumerate(id2rel):\n",
    "            rel2id[x] = i\n",
    "        return id2rel, rel2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = \"/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/datasets/data_with_marker_tacred.json\"\n",
    "data = json.load(open(file, \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(args.seed)\n",
    "print(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relation': 3, 'text': '[E21] lancaster county [E22] coroner dr. [E11] g. gary kirchner [E12] said the summary citations , filed by a humane society officer last week , were unwarranted .', 'tokens': [101, 30524, 10237, 2221, 30525, 22896, 2852, 1012, 30522, 1043, 1012, 5639, 11382, 11140, 3678, 30523, 2056, 1996, 12654, 22921, 1010, 6406, 2011, 1037, 23369, 2554, 2961, 2197, 2733, 1010, 2020, 4895, 9028, 17884, 2098, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 32, 'text': '[E11] vladimir ladyzhenskiy [E12] of russia died after she suffered [E21] a shock [E22] in the final of the spa world championship in heinola , a southern city of finland , on saturday .', 'tokens': [101, 30522, 8748, 3203, 27922, 6132, 3211, 2100, 30523, 1997, 3607, 2351, 2044, 2016, 4265, 30524, 1037, 5213, 30525, 1999, 1996, 2345, 1997, 1996, 12403, 2088, 2528, 1999, 2002, 5740, 2721, 1010, 1037, 2670, 2103, 1997, 6435, 1010, 2006, 5095, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 30, 'text': \"perugia , italy 2010-12-18 10:17:47 utc [E11] knox [E12] 's mother , [E21] edda mellas [E22] , was in court saturday .\", 'tokens': [101, 7304, 10440, 1010, 3304, 2230, 1011, 2260, 1011, 2324, 2184, 1024, 2459, 1024, 4700, 11396, 30522, 11994, 30523, 1005, 1055, 2388, 1010, 30524, 3968, 2850, 11463, 8523, 30525, 1010, 2001, 1999, 2457, 5095, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 10, 'text': 'he eventually landed in st paul , [E21] minn [E22] , where [E11] he [E12] became a cosmetologist and hair stylist .', 'tokens': [101, 2002, 2776, 5565, 1999, 2358, 2703, 1010, 30524, 8117, 2078, 30525, 1010, 2073, 30522, 2002, 30523, 2150, 1037, 2522, 6491, 18903, 10727, 1998, 2606, 2358, 8516, 2923, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 4, 'text': \"2007-07-11t00 :00:00 cuba has been under the rule of [E21] mariela [E22] 's father , [E11] raul castro [E12] , for the last year since fidel castro stepped down `` temporarily '' for health reasons .\", 'tokens': [101, 2289, 1011, 5718, 1011, 2340, 2102, 8889, 1024, 4002, 1024, 4002, 7394, 2038, 2042, 2104, 1996, 3627, 1997, 30524, 5032, 2721, 30525, 1005, 1055, 2269, 1010, 30522, 16720, 11794, 30523, 1010, 2005, 1996, 2197, 2095, 2144, 26000, 2140, 11794, 3706, 2091, 1036, 1036, 8184, 1005, 1005, 2005, 2740, 4436, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 31, 'text': 'in 1972 she returned to the u.s. with her daughter , katarina -lrb- who survives [E11] her [E12] along with her mother and a sister -rrb- , and enrolled in a doctoral program in political science at the [E21] massachusetts institute of technology [E22] .', 'tokens': [101, 1999, 3285, 2016, 2513, 2000, 1996, 1057, 1012, 1055, 1012, 2007, 2014, 2684, 1010, 29354, 11796, 1011, 1048, 15185, 1011, 2040, 13655, 30522, 2014, 30523, 2247, 2007, 2014, 2388, 1998, 1037, 2905, 1011, 25269, 2497, 1011, 1010, 1998, 8302, 1999, 1037, 11316, 2565, 1999, 2576, 2671, 2012, 1996, 30524, 4404, 2820, 1997, 2974, 30525, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 18, 'text': \"`` it 's december and there 's no need for heating , '' enthused piano , whose most famous projects include the pompidou centre in paris , kansai international airport in osaka , japan , and the [E11] menil collection [E12] in houston , [E21] texas [E22] .\", 'tokens': [101, 1036, 1036, 2009, 1005, 1055, 2285, 1998, 2045, 1005, 1055, 2053, 2342, 2005, 10808, 1010, 1005, 1005, 4372, 19877, 2098, 3682, 1010, 3005, 2087, 3297, 3934, 2421, 1996, 13433, 8737, 13820, 2226, 2803, 1999, 3000, 1010, 22827, 15816, 2248, 3199, 1999, 13000, 1010, 2900, 1010, 1998, 1996, 30522, 2273, 4014, 3074, 30523, 1999, 5395, 1010, 30524, 3146, 30525, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 34, 'text': 'the son of the [E11] pastor [E12] who was brutally killed inside her [E21] oklahoma [E22] church says her funeral will be another chance for his mother to spread the word of god .', 'tokens': [101, 1996, 2365, 1997, 1996, 30522, 9220, 30523, 2040, 2001, 23197, 2730, 2503, 2014, 30524, 5858, 30525, 2277, 2758, 2014, 6715, 2097, 2022, 2178, 3382, 2005, 2010, 2388, 2000, 3659, 1996, 2773, 1997, 2643, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 9, 'text': 'new york 2007-05-03 20:01:31 utc [E11] cana [E12] : [E21] http://wwwcanaconvocationorg/ [E22] a powerful anglican leader from nigeria is strengthening the network of us parishes he formed as a conservative alternative to the liberal-leaning episcopal church .', 'tokens': [101, 2047, 2259, 2289, 1011, 5709, 1011, 6021, 2322, 1024, 5890, 1024, 2861, 11396, 30522, 2064, 2050, 30523, 1024, 30524, 8299, 1024, 1013, 1013, 7479, 28621, 8663, 19152, 21759, 1013, 30525, 1037, 3928, 9437, 3003, 2013, 7387, 2003, 16003, 1996, 2897, 1997, 2149, 11600, 2002, 2719, 2004, 1037, 4603, 4522, 2000, 1996, 4314, 1011, 6729, 9134, 2277, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'relation': 28, 'text': \"blackstone 's shareholders -- among them [E11] fidelity investments [E12] , the mutual fund giant , and the [E21] ohio public employees retirement system [E22] -- have not been so fortunate .\", 'tokens': [101, 10823, 5524, 1005, 1055, 15337, 1011, 1011, 2426, 2068, 30522, 22625, 10518, 30523, 1010, 1996, 8203, 4636, 5016, 1010, 1998, 1996, 30524, 4058, 2270, 5126, 5075, 2291, 30525, 1011, 1011, 2031, 2025, 2042, 2061, 19590, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "data = data_sampler(args, seed=2021)\n",
    "list_data = []\n",
    "\n",
    "for steps, (training_data, valid_data, test_data, current_relations, historic_test_data, seen_relations) in enumerate(data):\n",
    "    \n",
    "    task_x = []\n",
    "    # print(current_relations)\n",
    "    # for relation in current_relations:\n",
    "    #     for sample in training_data[relation]:\n",
    "    #         task_x.append({\n",
    "    #             'relation': relation,\n",
    "    #             'text': sample['text']\n",
    "    #         })\n",
    "    # list_data.append(task_x)\n",
    "    \n",
    "        # for item in training_data[relation]:\n",
    "        #     if item['relation'] == 0:\n",
    "        #         print(item)\n",
    "    # if training_data[current_relations[0]][21]['relation'] == 0:\n",
    "    print(training_data[current_relations[0]][21])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_description = []\n",
    "\n",
    "\n",
    "x = {\n",
    "    'relation': 'per:religion',\n",
    "    'text': \"\"\"The relation \"per:religion\" identifies and associates an individual with their religious affiliation or beliefs. In the given examples, this relation is used to link people to their respective religions, showcasing how this information is relevant or contextual within the narrative.\n",
    "the \"per:religion\" relation is crucial for understanding how an individual's religious identity can influence, explain, or contextualize their actions, interactions, and roles within various narratives, whether these are legal, social, or political.\n",
    "\"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "list_description.append(x)\n",
    "print(len(list_description))\n",
    "\n",
    "import json\n",
    "json.dump(list_description, open('./description.json', 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relation name \"per:charges\" refers to a specific type of relationship between a person (E11, E12) and the charges or accusations (E21, E22) made against that person. This relationship is extracted from textual data where an individual's legal or criminal charges are explicitly mentioned. The examples provided illustrate various instances of how these charges can be represented in text, highlighting the diversity in the nature of the charges and the contexts in which they are presented.\n",
      "The \"per:charges\" relation captures the legal or criminal accusations made against individuals. This relation is vital for extracting information related to legal proceedings, criminal activities, and judicial sentences from textual sources. It can include a wide array of charges, from financial crimes and violent offenses to politically motivated actions. Additionally, the relation can provide insights into the legal characterizations of the actions (e.g., hate crimes), offering a deeper understanding of the context and severity of the charges. Understanding this relation is crucial for legal analyses, journalistic reporting, and historical documentation, as it directly ties individuals to specific actions that have legal implications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in list_description:\n",
    "    if item['relation'] == 'per:charges':\n",
    "        print(item['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "des1 = json.load(open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/description/all.json', 'r'))\n",
    "des2 = json.load(open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/description/new_description_2.json', 'r'))\n",
    "des3 = json.load(open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/description/new_description_3.json', 'r'))\n",
    "des4 = json.load(open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/description/new_description_4.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relation name \"per:city_of_birth\" refers to the specific type of relationship between a person (E11, E12) and the city or location where that person was born (E21, E22). This relationship is identified from textual contexts that explicitly mention the birthplace of an individual. The examples provided showcase instances where the city of birth is integral to the identity or background of the person mentioned, demonstrating the variety of ways this information can be presented in text.\n",
      "\n",
      "The \"per:city_of_birth\" relation is fundamental for understanding the geographical and cultural origins of individuals. It connects people to specific places, offering insights into their early life influences, cultural heritage, and the socio-economic conditions of their birthplaces. This relationship is crucial for biographical studies, historical research, and sociological analyses, as it provides a foundational aspect of a person's identity. Identifying the city of birth can also be important in the fields of genealogy, demographics, and even in legal contexts where place of birth can influence citizenship or legal rights. Understanding this relation enriches the narrative of individuals by anchoring their stories to specific geographical locations, thereby contributing to a more comprehensive understanding of their life contexts and influences.\n"
     ]
    }
   ],
   "source": [
    "# des_all = des1 + des2 + des3 + des4\n",
    "# len(des_all)\n",
    "\n",
    "# json.dump(des_all, open('/home/luungoc/Thesis - 2023.2/Thesis_NgocLT/description/all.json', 'w'), ensure_ascii=False)\n",
    "\n",
    "\n",
    "for item in des1:\n",
    "    if item['relation'] == 'per:city_of_birth':\n",
    "        print(item['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['per:cities_of_residence', 'per:other_family', 'org:founded', 'per:origin']\n",
    "['per:cause_of_death', 'org:dissolved', 'per:employee_of', 'org:member_of']\n",
    "['per:parents', 'per:alternate_names', 'org:top_members/employees', 'per:siblings']\n",
    "['per:stateorprovinces_of_residence', 'org:alternate_names', 'org:country_of_headquarters', 'per:country_of_birth']\n",
    "['per:children', 'per:date_of_birth', 'org:founded_by', 'per:countries_of_residence']\n",
    "['per:schools_attended', 'org:subsidiaries', 'org:members', 'org:political/religious_affiliation']\n",
    "['org:stateorprovince_of_headquarters', 'per:charges', 'per:stateorprovince_of_birth', 'per:title']\n",
    "['per:stateorprovince_of_death', 'org:number_of_employees/members', 'per:city_of_death', 'per:spouse']\n",
    "['org:website', 'per:age', 'per:city_of_birth', 'per:date_of_death']\n",
    "['org:shareholders', 'org:parents', 'org:city_of_headquarters', 'per:religion']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
